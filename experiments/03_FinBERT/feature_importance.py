# =========================================================
#  FINAL BATTLE: NVDA 10-Year News Analysis
#  Target: Verify News Importance with Recovered Data
# =========================================================

import pandas as pd
import numpy as np
import talib
import lightgbm as lgb
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import pipeline
from tqdm import tqdm
import torch
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score
import os
import sys

## --- 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ (ä¿®æ­£ç‰ˆ) ---
CSV_FILE = "nvda_news_fixed.csv"

print(f"ğŸ“‚ '{CSV_FILE}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
if not os.path.exists(CSV_FILE):
    print("âŒ ã‚¨ãƒ©ãƒ¼: ä¿®æ­£æ¸ˆã¿CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit()

df_news = pd.read_csv(CSV_FILE)

# â˜…ä¿®æ­£ç‚¹: format='mixed' ã§ã‚ã‚‰ã‚†ã‚‹å½¢å¼ã«å¯¾å¿œã•ã›ã‚‹
df_news['date'] = pd.to_datetime(df_news['date'], format='mixed', utc=True).dt.tz_localize(None)

print(f"âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(df_news)} ä»¶")
print(f"ğŸ“… æœŸé–“: {df_news['date'].min().date()} ã€œ {df_news['date'].max().date()}")

# --- ä»¥é™ã¯å¤‰æ›´ãªã— ---

# --- 2. FinBERTã§å…¨ä»¶ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° (GPUæ¨å¥¨) ---
print("\nğŸ§  AI (FinBERT) ãŒ10å¹´åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¡ç‚¹ä¸­...")

device = 0 if torch.cuda.is_available() else -1
print(f"  Device: {'GPU' if device == 0 else 'CPU'}")

classifier = pipeline("text-classification", model="ProsusAI/finbert", device=device, top_k=None)

sentiment_scores = []
headlines = df_news['headline'].tolist()

# ãƒãƒƒãƒå‡¦ç† (64ä»¶ãšã¤)
batch_size = 64
for i in tqdm(range(0, len(headlines), batch_size), desc="Scoring"):
    batch = headlines[i : i+batch_size]
    try:
        batch_clean = [str(text)[:512] for text in batch] # 512æ–‡å­—åˆ¶é™
        results = classifier(batch_clean)

        for res in results:
            score_dict = {x['label']: x['score'] for x in res}
            score = score_dict.get('positive', 0) - score_dict.get('negative', 0)
            sentiment_scores.append(score)
    except:
        sentiment_scores.extend([0] * len(batch))

df_news['news_score'] = sentiment_scores

# æ—¥æ¬¡é›†è¨ˆ (åŒæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯å¹³å‡)
daily_sentiment = df_news.groupby('date')['news_score'].mean()

# --- 3. æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾— (å®‰å…¨ç‰ˆ) ---
TARGET_TICKER = "NVDA"

# æ—¥ä»˜ç¯„å›²ã®ç¢ºèª
if not df_news.empty:
    start_dt = df_news['date'].min()
    end_dt = df_news['date'].max() + pd.Timedelta(days=5)
    print(f"ğŸ“… ãƒ‹ãƒ¥ãƒ¼ã‚¹æœŸé–“: {start_dt.date()} ã€œ {end_dt.date()}")
else:
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„å ´åˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æœŸé–“
    start_dt = "2011-01-01"
    end_dt = "2021-01-01"
    print("âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

print(f"ğŸ“Š {TARGET_TICKER} ã®æ ªä¾¡ã‚’å–å¾—ä¸­...")

try:
    # æ ªä¾¡å–å¾—
    df_price_raw = yf.download(TARGET_TICKER, start=start_dt, end=end_dt, interval="1d", progress=False)

    # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    if df_price_raw.empty:
        raise ValueError("æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚æœŸé–“ã¾ãŸã¯éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ã‚«ãƒ©ãƒ æ•´å½¢ (MultiIndexå¯¾ç­–)
    if isinstance(df_price_raw.columns, pd.MultiIndex):
        # Closeåˆ—ã ã‘ã‚’Seriesã¨ã—ã¦æŠ½å‡º
        df_price = df_price_raw['Close'].iloc[:, 0] if df_price_raw['Close'].shape[1] > 0 else df_price_raw['Close']
        # Volumeã‚‚åŒæ§˜ã«
        df_vol = df_price_raw['Volume'].iloc[:, 0] if df_price_raw['Volume'].shape[1] > 0 else df_price_raw['Volume']

        # High/Lowã‚‚
        df_high = df_price_raw['High'].iloc[:, 0]
        df_low = df_price_raw['Low'].iloc[:, 0]

    else:
        df_price = df_price_raw['Close']
        df_vol = df_price_raw['Volume']
        df_high = df_price_raw['High']
        df_low = df_price_raw['Low']

    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‰Šé™¤
    df_price.index = df_price.index.tz_localize(None)
    df_vol.index = df_vol.index.tz_localize(None)
    df_high.index = df_high.index.tz_localize(None)
    df_low.index = df_low.index.tz_localize(None)

    # çµåˆ (DataFrameä½œæˆ)
    df = pd.DataFrame({
        'Close': df_price,
        'High': df_high,
        'Low': df_low,
        'Volume': df_vol
    })

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã®ãƒãƒ¼ã‚¸
    # daily_sentiment ãŒã‚ã‚‹å ´åˆã®ã¿
    if 'daily_sentiment' in locals():
        df['news_score'] = df.index.map(daily_sentiment)
        df['news_score'] = df['news_score'].fillna(0) # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—æ—¥ã¯0

        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ã‚Šç‡ã®ç¢ºèª
        non_zero = (df['news_score'] != 0).sum()
        print(f"âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹åæ˜ æ—¥æ•°: {non_zero} æ—¥ / å…¨ {len(df)} æ—¥")
    else:
        print("âš ï¸ daily_sentiment ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã¯å…¨ã¦0ã«ãªã‚Šã¾ã™ã€‚")
        df['news_score'] = 0

    # --- 4. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° & æ¤œè¨¼ ---
    print("âš™ï¸ AIå­¦ç¿’æº–å‚™...")

    # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
    df['rsi'] = talib.RSI(df['Close'], 14)
    df['adx'] = talib.ADX(df['High'], df['Low'], df['Close'], 14)
    df['vol_change'] = df['Volume'].pct_change()
    df['return_1d'] = df['Close'].pct_change()

    # Target: ç¿Œæ—¥ä¸ŠãŒã‚‹ã‹ï¼Ÿ
    df['target'] = (df['Close'].shift(-1) > df['Close']).astype(int)

    df.dropna(inplace=True)
    print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)} è¡Œ")

    # LightGBM æ¤œè¨¼
    features = ['rsi', 'adx', 'return_1d', 'vol_change', 'news_score']
    tscv = TimeSeriesSplit(n_splits=5)
    importances = pd.DataFrame(index=features)
    acc_scores = []

    print("\n=== ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):
        X_train, y_train = df.iloc[train_idx][features], df.iloc[train_idx]['target']
        X_test, y_test = df.iloc[test_idx][features], df.iloc[test_idx]['target']

        model = lgb.LGBMClassifier(random_state=42, verbose=-1, n_estimators=100)
        model.fit(X_train, y_train)

        preds = model.predict(X_test)
        acc = accuracy_score(y_test, preds)
        acc_scores.append(acc)
        importances[f'Fold_{fold}'] = model.feature_importances_

        print(f"Fold {fold+1}: æ­£è§£ç‡ {acc:.2%}")

    print(f"\nå¹³å‡æ­£è§£ç‡: {np.mean(acc_scores):.2%}")

    # é‡è¦åº¦å¯è¦–åŒ–
    importances['Average'] = importances.mean(axis=1)
    importances = importances.sort_values('Average', ascending=False)

    print("\nã€ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘")
    print(importances['Average'])

    plt.figure(figsize=(10, 6))
    sns.barplot(x=importances['Average'], y=importances.index, palette='viridis')
    plt.title(f"Feature Importance: {TARGET_TICKER} (10-Year News Impact)")
    plt.xlabel("Importance")
    plt.grid(axis='x')
    plt.show()

except Exception as e:
    print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()







# =========================================================
#  Event-Driven Test: Does News Matter on "News Days"?
#  Target: Only rows where news_score != 0
# =========================================================

print("\n=== ğŸ“° ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ‰ãƒªãƒ–ãƒ³æ¤œè¨¼ (ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚‹æ—¥é™å®š) ===")

# ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚‹æ—¥ã ã‘æŠ½å‡º
df_event = df[df['news_score'] != 0].copy()
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_event)} è¡Œ")

# LightGBMã§å†æ¤œè¨¼
features = ['rsi', 'adx', 'return_1d', 'vol_change', 'news_score']
tscv = TimeSeriesSplit(n_splits=5)
importances_event = pd.DataFrame(index=features)
acc_scores_event = []

for fold, (train_idx, test_idx) in enumerate(tscv.split(df_event)):
    X_train, y_train = df_event.iloc[train_idx][features], df_event.iloc[train_idx]['target']
    X_test, y_test = df_event.iloc[test_idx][features], df_event.iloc[test_idx]['target']

    model = lgb.LGBMClassifier(random_state=42, verbose=-1, n_estimators=100)
    model.fit(X_train, y_train)

    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    acc_scores_event.append(acc)
    importances_event[f'Fold_{fold}'] = model.feature_importances_
    print(f"Fold {fold+1}: æ­£è§£ç‡ {acc:.2%}")

print(f"\nå¹³å‡æ­£è§£ç‡: {np.mean(acc_scores_event):.2%}")

# é‡è¦åº¦å¯è¦–åŒ–
importances_event['Average'] = importances_event.mean(axis=1)
importances_event = importances_event.sort_values('Average', ascending=False)

print("\nã€ã‚¤ãƒ™ãƒ³ãƒˆæ—¥é™å®šãƒ»é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘")
print(importances_event['Average'])

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_event['Average'], y=importances_event.index, palette='magma')
plt.title(f"Feature Importance (News Days Only)")
plt.xlabel("Importance")
plt.grid(axis='x')
plt.show()